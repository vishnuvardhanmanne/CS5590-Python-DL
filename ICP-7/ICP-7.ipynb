{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Copy of Untitled6.ipynb","provenance":[{"file_id":"1HxhgTCkPZsl-6Qp77_wRlgRCWhNBfVHj","timestamp":1583523256572}],"collapsed_sections":[],"authorship_tag":"ABX9TyM8a2WYFqQ9OlSNNGMMA8Iq"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"8NyhVRwy9BHB","colab_type":"code","colab":{}},"source":["# 1st and 2nd solution\n","import requests\n","from bs4 import BeautifulSoup\n","\n","url = 'https://en.wikipedia.org/wiki/Google'\n","res = requests.get(url)\n","html_page = res.content\n","soup = BeautifulSoup(html_page, 'html.parser')\n","text = soup.find_all(text=True)\n","\n","output = ''\n","blacklist = [\n","\t'[document]',\n","\t'noscript',\n","\t'header',\n","\t'html',\n","\t'meta',\n","\t'head', \n","\t'input',\n","\t'script',\n","]\n","\n","for t in text:\n","\tif t.parent.name not in blacklist:\n","\t\toutput += '{} '.format(t)\n","\n","print(output)\n","with open('input.txt', 'w') as f:\n","  print(output,file=f)\n","with open('input.txt', 'r') as l:\n","  print(l.read())\n","from google.colab import files\n","files.download('input.txt')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"N5W-31hl_69z","colab_type":"text"},"source":["# New Section"]},{"cell_type":"code","metadata":{"id":"Cs1QkyquAGIQ","colab_type":"code","colab":{}},"source":["#3(a) solution\n","import nltk\n","nltk.download('punkt')\n","stokens = nltk.sent_tokenize(output)\n","wtokens = nltk.word_tokenize(output)\n","print(\"Sentence Tokenization : \\n\", stokens)\n","print(\"Word Tokenization : \\n\", wtokens)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"h46BBI46CWhI","colab_type":"code","colab":{}},"source":["#3(b) solution\n","import nltk\n","nltk.download('averaged_perceptron_tagger')\n","text = nltk.word_tokenize(output)\n","tagged = nltk.pos_tag(text)\n","print(tagged)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"2HdHdoyQC9NI","colab_type":"code","colab":{}},"source":["#3(c) solution\n","# 1 -> LancasterStemmer\n","lStem = LancasterStemmer()\n","print(\"Lancaster Stemming : \\n\")\n","for tok in wtokens:\n","    print(lStem.stem(str(tok)))\n","\n","# 2 -> SnowBallStemmer\n","sStem = SnowballStemmer('english')\n","print(\"SnowBall Stemming : \\n\")\n","for tok in wtokens:\n","    print(sStem.stem(str(tok)))\n","\n","# 3 -> PorterStemmer\n","pStem = PorterStemmer()\n","print(\"Porter Stemming : \\n\")\n","for tok in wtokens:\n","    print(pStem.stem(str(tok)))\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"meYnmE-AHJ6c","colab_type":"code","colab":{}},"source":["#3(d) solution\n","import nltk\n","nltk.download('wordnet')\n","from nltk.stem import WordNetLemmatizer\n","lemmatizer = WordNetLemmatizer()\n","print(\"Lemmatization :\\n\")\n","for tok in wtokens:\n","    print(lemmatizer.lemmatize(str(tok)))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"YUht5ZI7H_b4","colab_type":"code","colab":{}},"source":["#3(e) solution\n","from nltk.util import ngrams\n","trigrams=ngrams(wtokens,3)\n","for t in trigrams:\n","  print(t)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"MCGvDvOqImrE","colab_type":"code","colab":{}},"source":["#3(f) solution\n","from nltk import wordpunct_tokenize, pos_tag, ne_chunk\n","nltk.download('maxent_ne_chunker')\n","nltk.download('words')\n","print(ne_chunk(pos_tag(wordpunct_tokenize(output))))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"WZ8GEPbTJ5p_","colab_type":"code","colab":{}},"source":["from sklearn.neighbors import KNeighborsClassifier\n","from nltk.corpus import stopwords\n","from sklearn.datasets import fetch_20newsgroups\n","from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n","from sklearn.feature_extraction.text import TfidfTransformer\n","from sklearn import metrics\n","from sklearn.pipeline import Pipeline\n","from sklearn.naive_bayes import MultinomialNB\n","\n","# 4(a) solution \n","\n","twenty_train = fetch_20newsgroups(subset='train', shuffle=True)\n","tfidf_Vect = TfidfVectorizer()\n","X_train_tfidf = tfidf_Vect.fit_transform(twenty_train.data)\n","clf = KNeighborsClassifier()\n","clf.fit(X_train_tfidf, twenty_train.target)\n","twenty_test = fetch_20newsgroups(subset='test', shuffle=True)\n","X_test_tfidf = tfidf_Vect.transform(twenty_test.data)\n","predicted = clf.predict(X_test_tfidf)\n","score = metrics.accuracy_score(twenty_test.target, predicted)\n","print(score)\n","\n","\n","\n","# 4(b) solution \n","\n","twenty_train = fetch_20newsgroups(subset='train', shuffle=True)\n","tfidf_Vect = TfidfVectorizer(ngram_range=(1,2))\n","X_train_tfidf = tfidf_Vect.fit_transform(twenty_train.data)\n","clf = MultinomialNB()\n","clf.fit(X_train_tfidf, twenty_train.target)\n","twenty_test = fetch_20newsgroups(subset='test', shuffle=True)\n","X_test_tfidf = tfidf_Vect.transform(twenty_test.data)\n","predicted = clf.predict(X_test_tfidf)\n","score = metrics.accuracy_score(twenty_test.target, predicted)\n","print(score)\n","\n","# 4(c) solution \n","\n","twenty_train = fetch_20newsgroups(subset='train', shuffle=True)\n","tfidf_Vect = TfidfVectorizer(stop_words='english')\n","X_train_tfidf = tfidf_Vect.fit_transform(twenty_train.data)\n","clf = MultinomialNB()\n","clf.fit(X_train_tfidf, twenty_train.target)\n","twenty_test = fetch_20newsgroups(subset='test', shuffle=True)\n","X_test_tfidf = tfidf_Vect.transform(twenty_test.data)\n","predicted = clf.predict(X_test_tfidf)\n","score = metrics.accuracy_score(twenty_test.target, predicted)\n","print(score)"],"execution_count":0,"outputs":[]}]}